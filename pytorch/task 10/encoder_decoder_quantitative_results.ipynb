{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import yaml\n",
    "import glob\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from kornia.metrics import psnr, ssim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # for cuda\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_files():\n",
    "    import google.colab\n",
    "    import zipfile\n",
    "\n",
    "    google.colab.drive.mount('/content/drive')\n",
    "    PROJECT_DIR = \"/content/drive/MyDrive/thesis/data/\"\n",
    "\n",
    "    zip_ref = zipfile.ZipFile(PROJECT_DIR + \"fiveK.zip\", 'r')\n",
    "    zip_ref.extractall(\".\")\n",
    "    zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "  extract_files()\n",
    "  config_path = \"/content/drive/MyDrive/thesis/config.yaml\"\n",
    "else:\n",
    "  config_path = \"../../config.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Load configuration\n",
    "    with open(config_path, 'r') as config_file:\n",
    "        config = yaml.safe_load(config_file)\n",
    "except:\n",
    "    raise FileNotFoundError(f\"Config file not found at path: {config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_type = config['unetmodel']['loss']\n",
    "depth = config['unetmodel']['depth']\n",
    "lambda_ = config['unetmodel']['contrastive_lambda']\n",
    "base_checkpoint_path = f\"{config['paths']['unetcheckpoints']}_five_classes_contrastive_{loss_type}_{depth}_{lambda_}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_best_checkpoint(checkpoint_dir):\n",
    "    # Check if the directory exists\n",
    "    if not os.path.exists(base_checkpoint_path):\n",
    "        print(f\"No directory found: {checkpoint_dir}\")\n",
    "        return None\n",
    "      # Get a list of all checkpoint files in the directory\n",
    "    checkpoint_files = glob.glob(os.path.join(checkpoint_dir, f'unet_*.pth'))\n",
    "\n",
    "    # sort the checkpoint files according to the epoch number\n",
    "    checkpoint_files.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "\n",
    "    # Check if any checkpoint files are present\n",
    "    if not checkpoint_files:\n",
    "        print(f\"No checkpoints found in the directory: {checkpoint_dir}\")\n",
    "        return None\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    epochs = []\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for checkpoint_file in checkpoint_files:\n",
    "        checkpoint = torch.load(checkpoint_file, map_location=torch.device(device))\n",
    "        epochs.append(checkpoint['epoch'])\n",
    "        train_losses.append(checkpoint['train_loss'])\n",
    "        val_losses.append(checkpoint['val_loss'])\n",
    "        if checkpoint['val_loss'] < best_val_loss:\n",
    "            best_val_loss = checkpoint['val_loss']\n",
    "            best_checkpoint = checkpoint\n",
    "\n",
    "    return best_checkpoint, epochs, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint, epochs, train_losses, val_losses = load_best_checkpoint(base_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(torch.nn.Module):\n",
    "    def __init__(self, inchannels, outchannels, downscale=False, upscale=False):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.down = torch.nn.MaxPool2d(2) if downscale else torch.nn.Identity()\n",
    "        self.conv1 = torch.nn.Conv2d(inchannels, outchannels, 3, padding=1)\n",
    "        self.bnorm1 = torch.nn.InstanceNorm2d(outchannels)\n",
    "        self.conv2 = torch.nn.Conv2d(outchannels, outchannels, 3, padding=1)\n",
    "        self.bnorm2 = torch.nn.InstanceNorm2d(outchannels)\n",
    "        self.up = torch.nn.Upsample(scale_factor=2) if upscale else torch.nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.down(x)\n",
    "        x = torch.nn.functional.relu(self.bnorm1(self.conv1(x)))\n",
    "        x = torch.nn.functional.relu(self.bnorm2(self.conv2(x)))\n",
    "        x = self.up(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(torch.nn.Module):\n",
    "    def __init__(self, classes, depth):\n",
    "        super(UNet, self).__init__()\n",
    "        self.encoder = torch.nn.ModuleList()\n",
    "        channels = [3] + [64 * (2 ** i) for i in range(depth)]\n",
    "        for i in range(depth):\n",
    "            self.encoder.append(ConvBlock(channels[i], channels[i + 1], downscale=(i > 0)))\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(classes, channels[-1])\n",
    "        self.bottleneck = ConvBlock(channels[-1], channels[-1], downscale=True, upscale=True)\n",
    "        \n",
    "        self.decoder = torch.nn.ModuleList()\n",
    "        self.linear = torch.nn.ModuleList()\n",
    "        channels[0] = 64\n",
    "        for i in range(depth - 1, -1, -1):\n",
    "            self.decoder.append(ConvBlock(2 * channels[i + 1], channels[i], upscale=(i > 0)))\n",
    "            self.linear.append(torch.nn.Linear(channels[-1], 2 * channels[i] if i > 0 else channels[i], bias=False))\n",
    "\n",
    "        self.output = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(channels[0], 3, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, label):\n",
    "        skip = []\n",
    "        for mod in self.encoder:\n",
    "            x = mod(x)\n",
    "            skip.append(x)\n",
    "        emb = self.embedding(label)\n",
    "        x = x + emb.unsqueeze(-1).unsqueeze(-1)\n",
    "        x = self.bottleneck(x)\n",
    "        for mod, linear in zip(self.decoder, self.linear):\n",
    "            y = skip.pop()\n",
    "            # add embedding with the decoder input\n",
    "            x = x + linear(emb).unsqueeze(-1).unsqueeze(-1)\n",
    "            x = torch.cat([x, y], 1)\n",
    "            x = mod(x)\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = config['unetmodel']['depth']\n",
    "net = UNet(classes=5, depth=3)\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = config['paths']['data']\n",
    "train_file = config['paths']['train']\n",
    "test_file = config['paths']['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tr = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.CenterCrop(224),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of class directories\n",
    "class_directories = ['expA','expB', 'expC', 'expD', 'expE']\n",
    "# raw data directory\n",
    "raw_dir = \"raw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FiveK(Dataset):\n",
    "    def __init__(self, data_dir, raw_data_dir, filename, transform=None):\n",
    "        super().__init__()\n",
    "        self.filename = filename\n",
    "        self.transform = transform\n",
    "\n",
    "        self.classname = self._extract_class_name(data_dir)\n",
    "        self.encode = {k: i for i, k in enumerate(class_directories)}\n",
    "\n",
    "\n",
    "        # Read the train.txt file and store the image paths\n",
    "        with open(self.filename) as f:\n",
    "            img_paths= []\n",
    "            raw_img_paths = []\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                img_paths.append(os.path.join(data_dir, line))\n",
    "                raw_img_paths.append(os.path.join(raw_data_dir, line))\n",
    "\n",
    "            self.image_paths = img_paths\n",
    "            self.raw_image_paths = raw_img_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.image_paths[index]\n",
    "        raw_image_path = self.raw_image_paths[index]\n",
    "        image = Image.open(image_path)\n",
    "        raw_image = Image.open(raw_image_path)\n",
    "        image = np.dstack((np.array(raw_image), np.array(image)))\n",
    "        label = self.encode[self.classname]\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        tr_raw_image = image[:3]\n",
    "        normalize = transforms.Normalize(mean=[0.2279, 0.2017, 0.1825], std=[0.1191, 0.1092, 0.1088])\n",
    "        tr_raw_image = normalize(tr_raw_image)\n",
    "        tr_image = image[3:]\n",
    "        tr_final_image = torch.cat((tr_raw_image, tr_image), 0)\n",
    "        return tr_final_image, label\n",
    "\n",
    "    def _extract_class_name(self, root_dir):\n",
    "        # Extract the class name from the root directory\n",
    "        class_name = os.path.basename(root_dir)\n",
    "        return class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(data_folder, txt_file, trasform=None):\n",
    "    # Create separate datasets for each class\n",
    "    datasets = []\n",
    "\n",
    "    for class_dir in class_directories:\n",
    "        class_train_dataset = FiveK(\n",
    "            data_dir=os.path.join(data_folder, class_dir),\n",
    "            raw_data_dir=os.path.join(data_folder, raw_dir),\n",
    "            filename=os.path.join(txt_file),\n",
    "            transform=trasform\n",
    "        )\n",
    "        datasets.append(class_train_dataset)\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = torch.utils.data.ConcatDataset(read_dataset(data_folder, test_file, test_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = DataLoader(val_dataset, batch_size=bs*2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "print(checkpoint['epoch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average PSNR: 21.551704025268556\n",
      "Average SSIM: 0.8064787864685059\n"
     ]
    }
   ],
   "source": [
    "psnrs = []\n",
    "ssims = []\n",
    "# calculate psnr for the validation dataset\n",
    "for inputs, labels in val_dataloader:\n",
    "    raw = inputs[:, :3]\n",
    "    gt = inputs[:, 3:]\n",
    "    raw = raw.to(device)\n",
    "    gt = gt.to(device)\n",
    "    labels = labels.to(device)\n",
    "    outputs = net(raw, labels)\n",
    "    psnr_val = psnr(outputs, gt, 1)\n",
    "    ssim_val = ssim(outputs, gt, 5).mean()\n",
    "    \n",
    "    ssims.append(ssim_val.item())\n",
    "    psnrs.append(psnr_val.item())\n",
    "print()\n",
    "print(f\"Average PSNR: {np.mean(psnrs)}\")\n",
    "print(f\"Average SSIM: {np.mean(ssims)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
