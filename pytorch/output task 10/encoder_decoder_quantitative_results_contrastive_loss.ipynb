{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlLZv24zPGI5",
        "outputId": "ac5ea6de-1a52-4f88-b182-9977b5a7b784"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kornia\n",
            "  Downloading kornia-0.7.3-py2.py3-none-any.whl (833 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m833.3/833.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting kornia-rs>=0.1.0 (from kornia)\n",
            "  Downloading kornia_rs-0.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from kornia) (24.1)\n",
            "Requirement already satisfied: torch>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from kornia) (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.15.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.9.1->kornia)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.9.1->kornia)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.9.1->kornia)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.9.1->kornia)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.9.1->kornia)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.9.1->kornia)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.9.1->kornia)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.9.1->kornia)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.9.1->kornia)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.9.1->kornia)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.9.1->kornia)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.9.1->kornia)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9.1->kornia) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9.1->kornia) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, kornia-rs, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, kornia\n",
            "Successfully installed kornia-0.7.3 kornia-rs-0.1.3 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "%pip install kornia\n",
        "import os\n",
        "import torch\n",
        "import yaml\n",
        "import glob\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from kornia.metrics import psnr, ssim\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "km_fBMD0PGI9"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # for cuda\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "17zqSW9yPGI_"
      },
      "outputs": [],
      "source": [
        "set_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UD7gxEPFPGI_"
      },
      "outputs": [],
      "source": [
        "def extract_files():\n",
        "    import google.colab\n",
        "    import zipfile\n",
        "\n",
        "    google.colab.drive.mount('/content/drive')\n",
        "    PROJECT_DIR = \"/content/drive/MyDrive/thesis/data/\"\n",
        "\n",
        "    zip_ref = zipfile.ZipFile(PROJECT_DIR + \"fiveK.zip\", 'r')\n",
        "    zip_ref.extractall(\".\")\n",
        "    zip_ref.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCoSC-byPGJA",
        "outputId": "635593a3-49ef-445b-9a6b-08d174689d38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "  extract_files()\n",
        "  config_path = \"/content/drive/MyDrive/thesis/config.yaml\"\n",
        "else:\n",
        "  config_path = \"../../config.yaml\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhjkZasyPGJA",
        "outputId": "5de702d3-0b13-4d78-8936-a2340527514d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PfFaWOGyPGJC"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    # Load configuration\n",
        "    with open(config_path, 'r') as config_file:\n",
        "        config = yaml.safe_load(config_file)\n",
        "except:\n",
        "    raise FileNotFoundError(f\"Config file not found at path: {config_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "DaJtCcCLPGJD"
      },
      "outputs": [],
      "source": [
        "loss_type = config['unetmodel']['loss']\n",
        "depth = config['unetmodel']['depth']\n",
        "lambda_ = config['unetmodel']['contrastive_lambda']\n",
        "base_checkpoint_path = f\"{config['paths']['unetcheckpoints']}_five_classes_contrastive_{loss_type}_{depth}_{lambda_}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6FzcJBfBPGJD"
      },
      "outputs": [],
      "source": [
        "def load_best_checkpoint(checkpoint_dir):\n",
        "    # Check if the directory exists\n",
        "    if not os.path.exists(base_checkpoint_path):\n",
        "        print(f\"No directory found: {checkpoint_dir}\")\n",
        "        return None\n",
        "      # Get a list of all checkpoint files in the directory\n",
        "    checkpoint_files = glob.glob(os.path.join(checkpoint_dir, f'unet_*.pth'))\n",
        "\n",
        "    # sort the checkpoint files according to the epoch number\n",
        "    checkpoint_files.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
        "\n",
        "    # Check if any checkpoint files are present\n",
        "    if not checkpoint_files:\n",
        "        print(f\"No checkpoints found in the directory: {checkpoint_dir}\")\n",
        "        return None\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    epochs = []\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    for checkpoint_file in checkpoint_files:\n",
        "        checkpoint = torch.load(checkpoint_file, map_location=torch.device(device))\n",
        "        epochs.append(checkpoint['epoch'])\n",
        "        train_losses.append(checkpoint['train_loss'])\n",
        "        val_losses.append(checkpoint['val_loss'])\n",
        "        if checkpoint['val_loss'] < best_val_loss:\n",
        "            best_val_loss = checkpoint['val_loss']\n",
        "            best_checkpoint = checkpoint\n",
        "\n",
        "    return best_checkpoint, epochs, train_losses, val_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3Tq5xr6KPGJE"
      },
      "outputs": [],
      "source": [
        "checkpoint, epochs, train_losses, val_losses = load_best_checkpoint(base_checkpoint_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "itdYDdozPGJE"
      },
      "outputs": [],
      "source": [
        "class ConvBlock(torch.nn.Module):\n",
        "    def __init__(self, inchannels, outchannels, downscale=False, upscale=False):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        self.down = torch.nn.MaxPool2d(2) if downscale else torch.nn.Identity()\n",
        "        self.conv1 = torch.nn.Conv2d(inchannels, outchannels, 3, padding=1)\n",
        "        self.bnorm1 = torch.nn.InstanceNorm2d(outchannels)\n",
        "        self.conv2 = torch.nn.Conv2d(outchannels, outchannels, 3, padding=1)\n",
        "        self.bnorm2 = torch.nn.InstanceNorm2d(outchannels)\n",
        "        self.up = torch.nn.Upsample(scale_factor=2) if upscale else torch.nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.down(x)\n",
        "        x = torch.nn.functional.relu(self.bnorm1(self.conv1(x)))\n",
        "        x = torch.nn.functional.relu(self.bnorm2(self.conv2(x)))\n",
        "        x = self.up(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "xfXUegbyPGJE"
      },
      "outputs": [],
      "source": [
        "class UNet(torch.nn.Module):\n",
        "    def __init__(self, classes, depth):\n",
        "        super(UNet, self).__init__()\n",
        "        self.encoder = torch.nn.ModuleList()\n",
        "        channels = [3] + [64 * (2 ** i) for i in range(depth)]\n",
        "        for i in range(depth):\n",
        "            self.encoder.append(ConvBlock(channels[i], channels[i + 1], downscale=(i > 0)))\n",
        "\n",
        "        self.embedding = torch.nn.Embedding(classes, channels[-1])\n",
        "        self.bottleneck = ConvBlock(channels[-1], channels[-1], downscale=True, upscale=True)\n",
        "\n",
        "        self.decoder = torch.nn.ModuleList()\n",
        "        self.linear = torch.nn.ModuleList()\n",
        "        channels[0] = 64\n",
        "        for i in range(depth - 1, -1, -1):\n",
        "            self.decoder.append(ConvBlock(2 * channels[i + 1], channels[i], upscale=(i > 0)))\n",
        "            self.linear.append(torch.nn.Linear(channels[-1], 2 * channels[i] if i > 0 else channels[i], bias=False))\n",
        "\n",
        "        self.output = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(channels[0], 3, 1),\n",
        "            torch.nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x, label):\n",
        "        skip = []\n",
        "        for mod in self.encoder:\n",
        "            x = mod(x)\n",
        "            skip.append(x)\n",
        "        emb = self.embedding(label)\n",
        "        x = x + emb.unsqueeze(-1).unsqueeze(-1)\n",
        "        x = self.bottleneck(x)\n",
        "        for mod, linear in zip(self.decoder, self.linear):\n",
        "            y = skip.pop()\n",
        "            # add embedding with the decoder input\n",
        "            x = x + linear(emb).unsqueeze(-1).unsqueeze(-1)\n",
        "            x = torch.cat([x, y], 1)\n",
        "            x = mod(x)\n",
        "        x = self.output(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "26DupRXJPGJF"
      },
      "outputs": [],
      "source": [
        "depth = config['unetmodel']['depth']\n",
        "net = UNet(classes=5, depth=3)\n",
        "net = net.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xx6a-Y9VPGJF",
        "outputId": "cfff7b03-3c0b-4bd3-a7e5-9ae02aec5e6d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "net.load_state_dict(checkpoint['state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "yULhALcrPGJF"
      },
      "outputs": [],
      "source": [
        "data_folder = config['paths']['data']\n",
        "train_file = config['paths']['train']\n",
        "test_file = config['paths']['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "tLxm5xxPPGJG"
      },
      "outputs": [],
      "source": [
        "test_tr = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.CenterCrop(224),\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "twXUM0AXPGJG"
      },
      "outputs": [],
      "source": [
        "# List of class directories\n",
        "class_directories = ['expA','expB', 'expC', 'expD', 'expE']\n",
        "# raw data directory\n",
        "raw_dir = \"raw\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "N0WfuiisPGJG"
      },
      "outputs": [],
      "source": [
        "class FiveK(Dataset):\n",
        "    def __init__(self, data_dir, raw_data_dir, filename, transform=None):\n",
        "        super().__init__()\n",
        "        self.filename = filename\n",
        "        self.transform = transform\n",
        "\n",
        "        self.classname = self._extract_class_name(data_dir)\n",
        "        self.encode = {k: i for i, k in enumerate(class_directories)}\n",
        "\n",
        "\n",
        "        # Read the train.txt file and store the image paths\n",
        "        with open(self.filename) as f:\n",
        "            img_paths= []\n",
        "            raw_img_paths = []\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                img_paths.append(os.path.join(data_dir, line))\n",
        "                raw_img_paths.append(os.path.join(raw_data_dir, line))\n",
        "\n",
        "            self.image_paths = img_paths\n",
        "            self.raw_image_paths = raw_img_paths\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image_path = self.image_paths[index]\n",
        "        raw_image_path = self.raw_image_paths[index]\n",
        "        image = Image.open(image_path)\n",
        "        raw_image = Image.open(raw_image_path)\n",
        "        image = np.dstack((np.array(raw_image), np.array(image)))\n",
        "        label = self.encode[self.classname]\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        tr_raw_image = image[:3]\n",
        "        normalize = transforms.Normalize(mean=[0.2279, 0.2017, 0.1825], std=[0.1191, 0.1092, 0.1088])\n",
        "        tr_raw_image = normalize(tr_raw_image)\n",
        "        tr_image = image[3:]\n",
        "        tr_final_image = torch.cat((tr_raw_image, tr_image), 0)\n",
        "        return tr_final_image, label\n",
        "\n",
        "    def _extract_class_name(self, root_dir):\n",
        "        # Extract the class name from the root directory\n",
        "        class_name = os.path.basename(root_dir)\n",
        "        return class_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "5J__gfH-PGJG"
      },
      "outputs": [],
      "source": [
        "def read_dataset(data_folder, txt_file, trasform=None):\n",
        "    # Create separate datasets for each class\n",
        "    datasets = []\n",
        "\n",
        "    for class_dir in class_directories:\n",
        "        class_train_dataset = FiveK(\n",
        "            data_dir=os.path.join(data_folder, class_dir),\n",
        "            raw_data_dir=os.path.join(data_folder, raw_dir),\n",
        "            filename=os.path.join(txt_file),\n",
        "            transform=trasform\n",
        "        )\n",
        "        datasets.append(class_train_dataset)\n",
        "    return datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "xubsj0B7PGJG"
      },
      "outputs": [],
      "source": [
        "val_dataset = torch.utils.data.ConcatDataset(read_dataset(data_folder, test_file, test_tr))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "HGalRWRAPGJH"
      },
      "outputs": [],
      "source": [
        "bs = 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "oIMDy-pBPGJH"
      },
      "outputs": [],
      "source": [
        "val_dataloader = DataLoader(val_dataset, batch_size=bs, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ab-9_w2dPGJH",
        "outputId": "d3554c3c-d15b-429e-fcbb-17a0891afaa4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "167\n"
          ]
        }
      ],
      "source": [
        "print(checkpoint['epoch'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Dey05OTPGJI",
        "outputId": "42452010-3542-4711-d2b2-903d2f5afb6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average PSNR: 20.329298348472523\n",
            "Average SSIM: 0.8565619309870199\n"
          ]
        }
      ],
      "source": [
        "psnrs = []\n",
        "ssims = []\n",
        "# calculate psnr for the validation dataset\n",
        "for inputs, labels in val_dataloader:\n",
        "    raw = inputs[:, :3]\n",
        "    gt = inputs[:, 3:]\n",
        "    raw = raw.to(device)\n",
        "    gt = gt.to(device)\n",
        "    labels = labels.to(device)\n",
        "    outputs = net(raw, labels)\n",
        "    psnr_val = psnr(outputs, gt, 1)\n",
        "    ssim_val = ssim(outputs, gt, 5).mean()\n",
        "\n",
        "    ssims.append(ssim_val.item())\n",
        "    psnrs.append(psnr_val.item())\n",
        "\n",
        "print(f\"Average PSNR: {np.mean(psnrs)}\")\n",
        "print(f\"Average SSIM: {np.mean(ssims)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dap4h0wSPGJI",
        "outputId": "3b599e25-ee4e-429e-8164-6d41e867e97e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5000\n"
          ]
        }
      ],
      "source": [
        "print(len(val_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "etOYHoXpnf1x"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}